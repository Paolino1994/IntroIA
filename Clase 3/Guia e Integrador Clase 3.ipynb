{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Guia Clase 3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyO2orhKTy/sBKKfRoYtwv+4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Paolino1994/IntroIA/blob/main/Clase%203/Guia%20e%20Integrador%20Clase%203.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn import preprocessing\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from functools import partial\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "hZZfHKapw2Oe"
      },
      "execution_count": 282,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecicio #1: Normalización\n",
        "\n",
        "Muchos algoritmos de Machine Learning necesitan datos de entrada centrados y normalizados. Una normalización habitual es el z-score, que implica restarle la media y dividir por el desvío a cada feature de mi dataset."
      ],
      "metadata": {
        "id": "WOjWTeIcwrj5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def zScore(X):\n",
        "  #X=np.array(X)\n",
        "  mean=X.mean(axis=0)\n",
        "  std=X.std(axis=0)\n",
        "  return (X-mean)/std"
      ],
      "metadata": {
        "id": "67Fxp9j-ws3F"
      },
      "execution_count": 283,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([1,80,3,55]).reshape(2,2)\n",
        "scaler = preprocessing.StandardScaler().fit(X)\n",
        "X_scaled = scaler.transform(X)\n",
        "if(np.allclose(zScore(X),X_scaled)):\n",
        "  print(\"Scaler is working Great\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVNjyaABy27z",
        "outputId": "55aeeb95-5716-4f2e-9fe1-fc58f518a1e7"
      },
      "execution_count": 284,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scaler is working Great\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecicio #2: Remover filas y columnas con NaNs en un dataset\n",
        "\n",
        "Dado un dataset, hacer una función que, utilizando numpy, filtre las columnas y las filas que tienen NaNs."
      ],
      "metadata": {
        "id": "zr6TYdnE0-Oj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def removeNANs(X):\n",
        "  nonNANRows=~np.isnan(X).any(axis=0)\n",
        "  X=X.T[nonNANRows].T\n",
        "  nonNANColumns=~np.isnan(X).any(axis=1)\n",
        "  X=X[nonNANColumns]\n",
        "  return X"
      ],
      "metadata": {
        "id": "4P8ql8mI1Fog"
      },
      "execution_count": 285,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([0,1,2,3,4,5,np.NaN,7,8]).reshape(3,3)\n",
        "print(X)\n",
        "removeNANs(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zYxRBZEA2GqE",
        "outputId": "3ea50f63-0cba-4a40-972b-8f4329c2dd2d"
      },
      "execution_count": 286,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  1.  2.]\n",
            " [ 3.  4.  5.]\n",
            " [nan  7.  8.]]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1., 2.],\n",
              "       [4., 5.],\n",
              "       [7., 8.]])"
            ]
          },
          "metadata": {},
          "execution_count": 286
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecicio #3: Reemplazar NaNs por la media de la columna\n",
        "\n",
        "Dado un dataset, hacer una función que utilizando numpy reemplace los NaNs por la media de la columna."
      ],
      "metadata": {
        "id": "HrcKmsYX3wCA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def replaceNANs(X):\n",
        "  means=np.nanmean(X, axis=0)\n",
        "  Nans = np.where(np.isnan(X))\n",
        "  X[Nans] = np.take(means, Nans[1]) \n",
        "  return X\n"
      ],
      "metadata": {
        "id": "niabrc9f3yEq"
      },
      "execution_count": 287,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array([0,1,2,3,4,5,np.NaN,7,8]).reshape(3,3)\n",
        "replaceNANs(X)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RHmkqvwP4PAA",
        "outputId": "3099c98b-0f0f-43c0-9fe3-325bcecd382e"
      },
      "execution_count": 288,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0. , 1. , 2. ],\n",
              "       [3. , 4. , 5. ],\n",
              "       [1.5, 7. , 8. ]])"
            ]
          },
          "metadata": {},
          "execution_count": 288
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ejecicio #4: Dado un dataset X separarlo en 70 / 20 / 10\n",
        "\n",
        "Como vimos en el ejercicio integrador, en problemas de Machine Learning es fundamental que separemos los datasets de n muestras, en 3 datasets de la siguiente manera:\n",
        "\n",
        "    Training dataset: los datos que utilizaremos para entrenar nuestros modelos. Ej: 70% de las muestras.\n",
        "    Validation dataset: los datos que usamos para calcular métricas y ajustar los hiperparámetros de nuestros modelos. Ej: 20% de las muestras.\n",
        "    Testing dataset: una vez que entrenamos los modelos y encontramos los hiperparámetros óptimos de los mísmos, el testing dataset se lo utiliza para computar las métricas finales de nuestros modelos y analizar cómo se comporta respecto a la generalización. Ej: 10% de las muestras.\n",
        "\n",
        "A partir de utilizar np.random.permutation, hacer un método que dado un dataset, devuelva los 3 datasets como nuevos numpy arrays."
      ],
      "metadata": {
        "id": "brILkN8d5a5A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def splitData(X,trainP=0.7,valP=0.2):\n",
        "  if(trainP+valP>1):\n",
        "    raise Exception('Sum is not %100')\n",
        "  Xrand=np.random.permutation(X)\n",
        "  trainQ=int(len(Xrand)*trainP)\n",
        "  valQ=int(len(Xrand)*valP)+trainQ\n",
        "  if(trainP+valP<1):\n",
        "    testQ=len(Xrand)\n",
        "    return Xrand[:trainQ],Xrand[trainQ:valQ],Xrand[valQ:testQ]\n",
        "  else:\n",
        "    return Xrand[:trainQ],Xrand[trainQ:]"
      ],
      "metadata": {
        "id": "x5nBASln5cfZ"
      },
      "execution_count": 371,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X=np.array(range(0,300)).reshape(100,3)\n",
        "for i in splitData(X):\n",
        "  print(i.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3H9qkCkK6O5C",
        "outputId": "005ebd8e-2390-47aa-83b5-434810690ee3"
      },
      "execution_count": 372,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(70, 3)\n",
            "(20, 3)\n",
            "(10, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "INTEGRADOR\n",
        "\n",
        "1.\tCargar los datos con objeto de clase Data (implementada por ustedes) con un método que cumpla esa función al pasarle la ruta. Hacer un split de los datos en train/test (usar 80/20)\n",
        "2.\tTratar los nans con al menos dos de las técnicas vistas en clase. (pasarían a tener dos datasets para comparar en lo que sigue)\n",
        "3.\tUtilizar PCA para quedarse con las 3 CP.  (de cada uno del punto 2, idealmente usen su implementación, pero pueden usar las librerías)\n",
        "4.\tCrear una clase métrica base y una clase MSE que herede es ella. (esto viene de ejercicios anteriores)\n",
        "5.\tCrear una clase modelo base y clase regresión lineal que herede de ella.  (esto viene de ejercicios anteirores)\n",
        "6.\tEntrenar la regresión lineal sobre train. Calcular MSE sobre validation. (para todas las variantes que hayan hecho en 2) y comparar.\n"
      ],
      "metadata": {
        "id": "jSUHttrd9PlW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AbstractMetric:\n",
        "    def __init__(self, **kwargs):\n",
        "        self.parameters = kwargs\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "def MSE(y,yHat):\n",
        "  return np.sum((y-yHat)**2)/len(y)\n",
        "\n",
        "class MSEClass (AbstractMetric):\n",
        "    def __init__(self,**kwargs):\n",
        "        AbstractMetric.__init__(self,**kwargs)\n",
        "    def __call__(self):\n",
        "        return MSE(self.parameters[\"y\"],self.parameters[\"yHat\"])\n",
        "\n",
        "def splitXY(X):\n",
        "  lenX=len(X)-1\n",
        "  return X[:,0:lenX],X[:,-1]"
      ],
      "metadata": {
        "id": "Sz31KOVj--RL"
      },
      "execution_count": 373,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AbstractModel:\n",
        "  def __init__(self, **kwargs):\n",
        "        self.parameters = kwargs\n",
        "\n",
        "  def __call__(self, *args, **kwargs):\n",
        "        pass\n",
        "\n",
        "def predict(a,b,X):\n",
        "  return a + np.sum(b * X,axis=1)\n",
        "\n",
        "def LinRegFunc(x_train,y_train):\n",
        "  # Iniciar parámetros a y b -> Para hacer con numpy\n",
        "  #np.random.seed(42)\n",
        "  np.random.seed(1234)\n",
        "  a = np.random.randn(1)\n",
        "  b = np.random.randn(1)\n",
        "  #print(a.shape,b.shape,x_train.shape)\n",
        "  lr = 0.000001  # constante de aprendizaje\n",
        "  n_epochs = 100000  # número de iteraciones\n",
        "  for epoch in range(n_epochs):\n",
        "      # Computar predicción del modelo\n",
        "      yhat =  np.sum(b* x_train,axis=1)\n",
        "      yhat =  a + yhat\n",
        "      # Calcular error cuadrático medio (ECM)\n",
        "      error = (y_train - yhat)\n",
        "      loss = MSEClass(y=y_train,yHat=yhat)\n",
        "      loss = loss()\n",
        "\n",
        "      # Computar gradiente\n",
        "      a_grad = -2 * error.mean()\n",
        "      b_grad = -2 * np.dot(error,x_train  ).mean()\n",
        "      # Actualizar parámetros\n",
        "      a = a - (lr * a_grad)\n",
        "      b = b - (lr * b_grad)\n",
        "      #print(a,b)\n",
        "  # Devolver predicción final\n",
        "\n",
        "  return partial(predict, a, b)\n",
        "# Calcular ECM final\n",
        "class LinReg (AbstractModel):\n",
        "    def __init__(self,**kwargs):\n",
        "        AbstractMetric.__init__(self,**kwargs)\n",
        "    def __call__(self):\n",
        "        return LinRegFunc(self.parameters[\"x\"],self.parameters[\"y\"])"
      ],
      "metadata": {
        "id": "3zyHIUKaAHPU"
      },
      "execution_count": 424,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Data:\n",
        "  def __init__(self,path):\n",
        "    self.data = np.genfromtxt(path, delimiter=';')\n",
        "  \n",
        "  def removeNANs(self):\n",
        "    self.data = removeNANs(self.data)\n",
        "  \n",
        "  def replaceNANs(self):\n",
        "    self.data = replaceNANs(self.data)\n",
        "\n",
        "  def splitData(self):\n",
        "    train,test=splitData(self.data,0.8,0.2)\n",
        "    x_train,y_train=splitXY(train)\n",
        "    x_test,y_test=splitXY(test)\n",
        "    return x_train,y_train,x_test,y_test"
      ],
      "metadata": {
        "id": "BR7wjAOdhhNX"
      },
      "execution_count": 425,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load Data\n",
        "#DAN -> Data with NaNs replaces by Average\n",
        "#DNN -> Data with columns eliminated if had NaNs\n",
        "\n",
        "path=\"clase3v2.csv\"\n",
        "\n",
        "myDataNoNans=Data(path)\n",
        "myDataNoNans.removeNANs()\n",
        "myDataAvgNans=Data(path)\n",
        "myDataAvgNans.replaceNANs()\n",
        "\n",
        "#Separate\n",
        "DNN_trainX,DNN_trainY,DNN_valX,DNN_valY=myDataNoNans.splitData()\n",
        "DAN_trainX,DAN_trainY,DAN_valX,DAN_valY=myDataAvgNans.splitData()\n",
        "\n",
        "#PCA for DNN\n",
        "pca = PCA(0.95)\n",
        "pca.fit(DNN_trainX)\n",
        "\n",
        "DNN_trainX_PCA = pca.transform(DNN_trainX)\n",
        "DNN_valX_PCA = pca.transform(DNN_valX)\n",
        "print(f\"For DNN Reduced dimensions from {DNN_trainX.shape[1]} to {DNN_trainX_PCA.shape[1]}\")\n",
        "\n",
        "#PCA for DAN\n",
        "\n",
        "pca = PCA(0.95)\n",
        "pca.fit(DAN_trainX)\n",
        "\n",
        "DAN_trainX_PCA = pca.transform(DAN_trainX)\n",
        "DAN_valX_PCA = pca.transform(DAN_valX)\n",
        "print(f\"For DAN Reduced dimensions from {DAN_trainX.shape[1]} to {DAN_trainX_PCA.shape[1]}\")"
      ],
      "metadata": {
        "id": "vHPd9KTjiNjN",
        "outputId": "484189d2-ad37-4e36-e5b5-b2283c1a879e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 426,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For DNN Reduced dimensions from 5 to 2\n",
            "For DAN Reduced dimensions from 7 to 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict using No NANs\n",
        "predictorOriginal=LinReg(x=DNN_trainX,y=DNN_trainY)()\n",
        "yHatOriginal=predictorOriginal(DNN_valX)\n",
        "errorOriginal=MSEClass(y=DNN_valY,yHat=yHatOriginal)\n",
        "\n",
        "predictorPCA=LinReg(x=DNN_trainX_PCA,y=DNN_trainY)()\n",
        "yHatPCA=predictorPCA(DNN_valX_PCA)\n",
        "errorPCA=MSEClass(y=DNN_valY,yHat=yHatPCA)\n",
        "\n",
        "print(f\"Error using DNN on the original Dataset is {errorOriginal()}\")\n",
        "print(f\"Error using DNN on the PCa Dataset is {errorPCA()}\")"
      ],
      "metadata": {
        "id": "wPnnAYRXbyc7",
        "outputId": "934ed580-ac17-4e02-a392-98e010891c8b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 427,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error using DNN on the original Dataset is 17.703632973067545\n",
            "Error using DNN on the PCa Dataset is 58.47953794438357\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Predict using AVG NANs\n",
        "predictorOriginal=LinReg(x=DAN_trainX,y=DAN_trainY)()\n",
        "yHatOriginal=predictorOriginal(DAN_valX)\n",
        "errorOriginal=MSEClass(y=DAN_valY,yHat=yHatOriginal)\n",
        "\n",
        "predictorPCA=LinReg(x=DAN_trainX_PCA,y=DAN_trainY)()\n",
        "yHatPCA=predictorPCA(DAN_valX_PCA)\n",
        "errorPCA=MSEClass(y=DAN_valY,yHat=yHatPCA)\n",
        "\n",
        "print(f\"Error using DAN on the original Dataset is {errorOriginal()}\")\n",
        "print(f\"Error using DAN on the PCa Dataset is {errorPCA()}\")"
      ],
      "metadata": {
        "id": "5aF07BvKkjh0",
        "outputId": "59e93ecf-afb0-4bc5-8a11-d749556096f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 428,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error using DAN on the original Dataset is 27.034226256089084\n",
            "Error using DAN on the PCa Dataset is 41.18685608402905\n"
          ]
        }
      ]
    }
  ]
}